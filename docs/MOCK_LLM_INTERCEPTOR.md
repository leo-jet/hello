# Mock LLM Models HTTP Interceptor

## üé≠ Description

Interceptor HTTP qui simule les r√©ponses API pour l'endpoint `/api/llm-models`.
Permet de d√©velopper et tester l'application sans backend r√©el.

## üì¶ Emplacement

```
src/app/interceptors/mock-llm-models.interceptor.ts
```

## üîß Configuration

L'interceptor est activ√© dans `app.config.ts` :

```typescript
import { mockLlmModelsInterceptor } from './interceptors/mock-llm-models.interceptor';

export const appConfig: ApplicationConfig = {
  providers: [
    provideHttpClient(withInterceptors([mockLlmModelsInterceptor])),
    // ...
  ]
};
```

## üìä Donn√©es mock√©es

L'interceptor retourne 4 mod√®les LLM :

```typescript
[
  {
    id: 'gpt-4',
    name: 'GPT-4',
    provider: 'OpenAI',
    description: 'Mod√®le avanc√©',
    maxTokens: 8192,
    isAvailable: true
  },
  {
    id: 'gpt-5',
    name: 'GPT-5 Pro',
    provider: 'OpenAI',
    description: 'Avec raisonnement',
    maxTokens: 16384,
    isAvailable: true
  },
  {
    id: 'claude-3',
    name: 'Claude 3',
    provider: 'Anthropic',
    description: 'Anthropic',
    maxTokens: 100000,
    isAvailable: true
  },
  {
    id: 'gemini-pro',
    name: 'Gemini Pro',
    provider: 'Google',
    description: 'Multimodal',
    maxTokens: 32768,
    isAvailable: true
  }
]
```

## üöÄ Fonctionnement

### 1. Interception automatique

```typescript
// Dans n'importe quel service
this.http.get<LlmModel[]>('/api/llm-models')
```

L'interceptor d√©tecte l'URL et retourne automatiquement les donn√©es mock√©es.

### 2. Simulation r√©aliste

- ‚úÖ **D√©lai** : 500ms pour simuler la latence r√©seau
- ‚úÖ **HttpResponse** : R√©ponse HTTP compl√®te (status 200)
- ‚úÖ **Observable** : Retourne un Observable comme une vraie requ√™te
- ‚úÖ **Console log** : Log "üé≠ Mock intercept√©" pour le debugging

### 3. Transparent

Les autres requ√™tes HTTP passent normalement :

```typescript
// Cette requ√™te n'est PAS intercept√©e
this.http.get('/api/users') // ‚Üí va vers le vrai backend
```

## üéØ Utilisation avec le Store

```typescript
import { Component, OnInit } from '@angular/core';
import { Store } from '@ngrx/store';
import * as ChatActions from '@app/store/chat';

@Component({...})
export class MyComponent implements OnInit {
  constructor(private store: Store) {}

  ngOnInit() {
    // Dispatch l'action
    this.store.dispatch(ChatActions.loadLlmModels());
    
    // 1. L'Effect intercepte l'action
    // 2. Appelle ChatModeService.loadLlmModels()
    // 3. Le service fait http.get('/api/llm-models')
    // 4. L'interceptor retourne les donn√©es mock√©es
    // 5. Le store est mis √† jour avec les mod√®les
  }
}
```

## üîÑ Flow complet

```
Component
  ‚Üì dispatch(loadLlmModels())
ChatEffects
  ‚Üì switchMap
ChatModeService.loadLlmModels()
  ‚Üì http.get('/api/llm-models')
mockLlmModelsInterceptor
  ‚Üì return Observable.of(mockData)
ChatEffects
  ‚Üì dispatch(loadLlmModelsSuccess({ models }))
ChatReducer
  ‚Üì update state
Component (via selectors)
  ‚úì Affiche les mod√®les
```

## üõ†Ô∏è Personnalisation

### Modifier les donn√©es

√âditez `mock-llm-models.interceptor.ts` :

```typescript
const mockModels: LlmModel[] = [
  {
    id: 'mon-modele',
    name: 'Mon Mod√®le Custom',
    provider: 'MaCompany',
    description: 'Description personnalis√©e',
    maxTokens: 4096,
    isAvailable: true
  },
  // ...
];
```

### Modifier le d√©lai

```typescript
return of(mockResponse).pipe(delay(1000)); // 1 seconde
```

### Simuler une erreur

```typescript
if (req.url.includes('/api/llm-models')) {
  // Simuler une erreur 500
  return throwError(() => new Error('Erreur serveur simul√©e')).pipe(
    delay(500)
  );
}
```

### D√©sactiver le mock

Dans `app.config.ts`, retirez l'interceptor :

```typescript
// Avant (avec mock)
provideHttpClient(withInterceptors([mockLlmModelsInterceptor]))

// Apr√®s (sans mock - utilise le vrai backend)
provideHttpClient()
```

## üß™ Test du mock

### Console

Ouvrez la console du navigateur, vous devriez voir :

```
üé≠ Mock intercept√©: /api/llm-models
```

### Redux DevTools

1. Ouvrez Redux DevTools
2. Dispatch `loadLlmModels`
3. Observez la s√©quence :
   - `[Chat] Load LLM Models`
   - `[Chat] Load LLM Models Success` (avec les 4 mod√®les)

### Network Tab

Dans l'onglet Network, vous ne verrez **PAS** de vraie requ√™te HTTP car elle est intercept√©e.

## üì± Composant de d√©monstration

Un composant exemple est disponible :

```
src/app/components/model-selector-example/model-selector-example.component.ts
```

Pour l'utiliser, ajoutez-le √† une route :

```typescript
{
  path: 'models-demo',
  component: ModelSelectorExampleComponent
}
```

Puis visitez : `http://localhost:4200/models-demo`

## ‚ö†Ô∏è Production

**IMPORTANT** : N'oubliez pas de retirer le mock en production !

```typescript
// app.config.ts
import { environment } from '@env/environment';

export const appConfig: ApplicationConfig = {
  providers: [
    // Utiliser le mock seulement en d√©veloppement
    provideHttpClient(
      withInterceptors(
        environment.production ? [] : [mockLlmModelsInterceptor]
      )
    ),
  ]
};
```

## üéì Avantages

‚úÖ **D√©veloppement ind√©pendant** - Pas besoin de backend  
‚úÖ **Tests rapides** - R√©ponses instantan√©es  
‚úÖ **Donn√©es contr√¥l√©es** - Testez tous les cas  
‚úÖ **D√©mos** - Pr√©sentations sans serveur  
‚úÖ **CI/CD** - Tests automatis√©s sans d√©pendances  

## üîó Ressources

- [Angular HTTP Interceptors](https://angular.dev/guide/http/interceptors)
- [RxJS of()](https://rxjs.dev/api/index/function/of)
- [RxJS delay()](https://rxjs.dev/api/operators/delay)
