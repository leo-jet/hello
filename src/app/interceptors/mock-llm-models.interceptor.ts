import { HttpInterceptorFn, HttpResponse } from '@angular/common/http';
import { of } from 'rxjs';
import { delay } from 'rxjs/operators';
import { LlmModel } from '@app/models';

/**
 * Mock HTTP Interceptor pour les appels API LLM Models
 * Simule une r√©ponse API avec des donn√©es de test
 */
export const mockLlmModelsInterceptor: HttpInterceptorFn = (req, next) => {
  // Intercepter uniquement les appels √† /api/llm-models
  if (req.url.includes('/api/llm-models') && req.method === 'GET') {
    console.log('üé≠ Mock intercept√©:', req.url);

    // Donn√©es mock
    const mockModels: LlmModel[] = [
      {
        id: 'gpt-4',
        name: 'GPT-4 sddfdfffdfdfdfdddf',
        provider: 'OpenAI',
        description: 'Mod√®le avanc√©',
        maxTokens: 8192,
        isAvailable: true,
        has_reasoning: false,
        reasoning_level: []
      },
      {
        id: 'gpt-5',
        name: 'GPT-5 Pro',
        provider: 'OpenAI',
        description: 'Avec raisonnement',
        maxTokens: 16384,
        isAvailable: true,
        has_reasoning: true,
        reasoning_level: ['low', 'medium', 'high']
      },
      {
        id: 'claude-3',
        name: 'Claude 3',
        provider: 'Anthropic',
        description: 'Anthropic',
        maxTokens: 100000,
        isAvailable: true,
        has_reasoning: false,
        reasoning_level: []
      },
      {
        id: 'gemini-pro',
        name: 'Gemini Pro',
        provider: 'Google',
        description: 'Multimodal',
        maxTokens: 32768,
        isAvailable: true,
        has_reasoning: false,
        reasoning_level: []
      }
    ];

    // Cr√©er une r√©ponse HTTP mock√©e
    const mockResponse = new HttpResponse({
      status: 200,
      body: mockModels
    });

    // Retourner un Observable avec un d√©lai simul√© (500ms)
    return of(mockResponse).pipe(delay(500));
  }

  // Pour toutes les autres requ√™tes, continuer normalement
  return next(req);
};
